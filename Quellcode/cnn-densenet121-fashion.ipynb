{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:53.833547Z",
     "start_time": "2024-08-01T14:27:53.809541700Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:54.880782300Z",
     "start_time": "2024-08-01T14:27:54.868779500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:56.060048100Z",
     "start_time": "2024-08-01T14:27:56.001034400Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.FashionMNIST(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:56.733199200Z",
     "start_time": "2024-08-01T14:27:56.652180300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x16623cf2870>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:57.308328Z",
     "start_time": "2024-08-01T14:27:57.285322300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_featuremaps=64, bn_size=4, drop_rate=0, num_classes=10, memory_efficient=False,\n",
    "                 grayscale=GRAYSCALE):\n",
    "\n",
    "        super(DenseNet121, self).__init__()\n",
    "\n",
    "        if grayscale:\n",
    "            in_channels=1\n",
    "        else:\n",
    "            in_channels=3\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(in_channels=in_channels, out_channels=num_init_featuremaps,\n",
    "                                kernel_size=3, stride=1,\n",
    "                                padding=1, bias=False)), # bias is redundant when using batchnorm\n",
    "            ('norm0', nn.BatchNorm2d(num_features=num_init_featuremaps)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        num_features = num_init_featuremaps\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        logits = self.classifier(out)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:58.089503700Z",
     "start_time": "2024-08-01T14:27:57.909463Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = DenseNet121(num_classes=NUM_CLASSES, grayscale=GRAYSCALE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keyboard\n",
    "import time\n",
    "\n",
    "def simulate_key_press(key):\n",
    "    keyboard.press(key)\n",
    "    time.sleep(0.1)  # Halte die Taste für 0.1 Sekunden gedrückt\n",
    "    keyboard.release(key)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:59.087727400Z",
     "start_time": "2024-08-01T14:27:59.068723200Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:27:59.933916900Z",
     "start_time": "2024-08-01T14:27:59.917914Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_acc(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    model.eval()\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        assert predicted_labels.size() == targets.size()\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:50:42.598202500Z",
     "start_time": "2024-08-01T14:50:21.074367200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/001, Batch: 001, Batch Time: 366.08 ms\n",
      "Epoch: 001/001, Batch: 002, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 003, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 004, Batch Time: 77.02 ms\n",
      "Epoch: 001/001, Batch: 005, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 006, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 007, Batch Time: 86.02 ms\n",
      "Epoch: 001/001, Batch: 008, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 009, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 010, Batch Time: 77.02 ms\n",
      "Epoch: 001/001, Batch: 011, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 012, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 013, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 014, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 015, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 016, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 017, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 018, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 019, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 020, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 021, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 022, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 023, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 024, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 025, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 026, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 027, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 028, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 029, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 030, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 031, Batch Time: 77.02 ms\n",
      "Epoch: 001/001, Batch: 032, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 033, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 034, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 035, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 036, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 037, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 038, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 039, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 040, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 041, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 042, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 043, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 044, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 045, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 046, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 047, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 048, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 049, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 050, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 051, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 052, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 053, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 054, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 055, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 056, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 057, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 058, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 059, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 060, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 061, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 062, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 063, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 064, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 065, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 066, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 067, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 068, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 069, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 070, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 071, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 072, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 073, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 074, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 075, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 076, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 077, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 078, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 079, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 080, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 081, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 082, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 083, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 084, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 085, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 086, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 087, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 088, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 089, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 090, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 091, Batch Time: 78.02 ms\n",
      "Epoch: 001/001, Batch: 092, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 093, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 094, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 095, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 096, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 097, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 098, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 099, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 100, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 101, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 102, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 103, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 104, Batch Time: 71.01 ms\n",
      "Epoch: 001/001, Batch: 105, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 106, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 107, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 108, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 109, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 110, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 111, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 112, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 113, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 114, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 115, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 116, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 117, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 118, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 119, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 120, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 121, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 122, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 123, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 124, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 125, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 126, Batch Time: 70.01 ms\n",
      "Epoch: 001/001, Batch: 127, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 128, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 129, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 130, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 131, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 132, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 133, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 134, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 135, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 136, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 137, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 138, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 139, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 140, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 141, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 142, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 143, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 144, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 145, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 146, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 147, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 148, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 149, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 150, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 151, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 152, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 153, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 154, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 155, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 156, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 157, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 158, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 159, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 160, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 161, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 162, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 163, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 164, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 165, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 166, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 167, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 168, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 169, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 170, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 171, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 172, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 173, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 174, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 175, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 176, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 177, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 178, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 179, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 180, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 181, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 182, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 183, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 184, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 185, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 186, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 187, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 188, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 189, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 190, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 191, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 192, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 193, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 194, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 195, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 196, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 197, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 198, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 199, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 200, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 201, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 202, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 203, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 204, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 205, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 206, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 207, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 208, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 209, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 210, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 211, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 212, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 213, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 214, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 215, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 216, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 217, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 218, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 219, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 220, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 221, Batch Time: 70.02 ms\n",
      "Epoch: 001/001, Batch: 222, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 223, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 224, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 225, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 226, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 227, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 228, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 229, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 230, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 231, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 232, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 233, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 234, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 235, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 236, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 237, Batch Time: 72.02 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001660B96FF70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\tom\\dev\\monitoring\\pythonProject\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 23\u001B[0m\n\u001B[0;32m     20\u001B[0m cost \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, targets)\n\u001B[0;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 23\u001B[0m \u001B[43mcost\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m### UPDATE MODEL PARAMETERS\u001B[39;00m\n\u001B[0;32m     26\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mC:\\tom\\dev\\monitoring\\pythonProject\\.venv\\lib\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\tom\\dev\\monitoring\\pythonProject\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "simulate_key_press('p')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "                \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d}\\n')\n",
    "            \n",
    "total_training_time = (time.time() - start_time)\n",
    "\n",
    "total_accuracy = compute_acc(model, valid_loader, device=DEVICE)\n",
    "print(f'Total Training Time: {total_training_time:.2f} sec')\n",
    "print(f'Total Training Time: {total_accuracy:.2f} %')\n",
    "time.sleep(1)\n",
    "simulate_key_press('p')"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(93.0000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "#Zum loggen der Trainingslaufzeit und accuracy  des Models\n",
    "\n",
    "def add_row_to_csv(filename, datensatz_name, frequency, voltage, runtime, accuracy):\n",
    "    # Daten für die neue Zeile\n",
    "    new_row = {'Datensatz': datensatz_name, 'Frequenz': frequency, 'voltage': voltage, 'Laufzeit': runtime, 'Accuracy': accuracy}\n",
    "    \n",
    "    # Überprüfe, ob die Datei existiert\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    # Öffne die CSV-Datei im Modus 'a' (append), um Werte hinzuzufügen\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        # Erstelle einen CSV-Writer, wenn die Datei neu erstellt wird\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Datensatz', 'Frequenz', 'voltage', 'Laufzeit', 'Accuracy'])\n",
    "        \n",
    "        # Schreibe die Kopfzeile, falls die Datei neu erstellt wurde\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Schreibe die neue Zeile in die CSV-Datei\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "filename = 'densenet121_fashion_spannung.csv'\n",
    "datensatz_name = 'fashion'\n",
    "frequency = '3600'\n",
    "voltage = '1.1'\n",
    "runtime = round(total_training_time, 2)\n",
    "print(total_accuracy)\n",
    "accuracy = round(total_accuracy.item(), 2)\n",
    "\n",
    "add_row_to_csv(filename, datensatz_name, frequency, voltage, runtime, accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T14:12:11.630932600Z",
     "start_time": "2024-06-06T14:12:11.617929500Z"
    }
   },
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paaeEQHQj5xC"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
