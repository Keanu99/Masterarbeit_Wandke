{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:22.079249800Z",
     "start_time": "2024-08-01T14:29:18.558460200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:22.767405700Z",
     "start_time": "2024-08-01T14:29:22.742398800Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:27.986576900Z",
     "start_time": "2024-08-01T14:29:27.969572900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:30.451130700Z",
     "start_time": "2024-08-01T14:29:29.003806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:31.031260300Z",
     "start_time": "2024-08-01T14:29:30.991251700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x163e4fa6870>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(DEVICE)\n",
    "torch.manual_seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:31.853445Z",
     "start_time": "2024-08-01T14:29:31.826439500Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "def _bn_function_factory(norm, relu, conv):\n",
    "    def bn_function(*inputs):\n",
    "        concated_features = torch.cat(inputs, 1)\n",
    "        bottleneck_output = conv(relu(norm(concated_features)))\n",
    "        return bottleneck_output\n",
    "\n",
    "    return bn_function\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1,\n",
    "                                           bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1,\n",
    "                                           bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "    def forward(self, *prev_features):\n",
    "        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n",
    "        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n",
    "            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n",
    "        else:\n",
    "            bottleneck_output = bn_function(*prev_features)\n",
    "        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate,\n",
    "                                     training=self.training)\n",
    "        return new_features\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Module):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(\n",
    "                num_input_features + i * growth_rate,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient,\n",
    "            )\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "    def forward(self, init_features):\n",
    "        features = [init_features]\n",
    "        for name, layer in self.named_children():\n",
    "            new_features = layer(*features)\n",
    "            features.append(new_features)\n",
    "        return torch.cat(features, 1)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet121(nn.Module):\n",
    "\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_featuremaps=64, bn_size=4, drop_rate=0, num_classes=10, memory_efficient=False,\n",
    "                 grayscale=False):\n",
    "\n",
    "        super(DenseNet121, self).__init__()\n",
    "\n",
    "        if grayscale:\n",
    "            in_channels=1\n",
    "        else:\n",
    "            in_channels=3\n",
    "        \n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(in_channels=in_channels, out_channels=num_init_featuremaps,\n",
    "                                kernel_size=7, stride=2,\n",
    "                                padding=3, bias=False)), # bias is redundant when using batchnorm\n",
    "            ('norm0', nn.BatchNorm2d(num_features=num_init_featuremaps)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
    "        ]))\n",
    "\n",
    "        num_features = num_init_featuremaps\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(\n",
    "                num_layers=num_layers,\n",
    "                num_input_features=num_features,\n",
    "                bn_size=bn_size,\n",
    "                growth_rate=growth_rate,\n",
    "                drop_rate=drop_rate,\n",
    "                memory_efficient=memory_efficient\n",
    "            )\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features,\n",
    "                                    num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        logits = self.classifier(out)\n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        return logits, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:32.815661800Z",
     "start_time": "2024-08-01T14:29:32.561604Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = DenseNet121(num_classes=NUM_CLASSES, grayscale=GRAYSCALE)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import keyboard\n",
    "import time\n",
    "\n",
    "def simulate_key_press(key):\n",
    "    keyboard.press(key)\n",
    "    time.sleep(0.1)  # Halte die Taste für 0.1 Sekunden gedrückt\n",
    "    keyboard.release(key)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:33.904907Z",
     "start_time": "2024-08-01T14:29:33.886902100Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T14:29:34.577057500Z",
     "start_time": "2024-08-01T14:29:34.545051700Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_acc(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    model.eval()\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        assert predicted_labels.size() == targets.size()\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7",
    "ExecuteTime": {
     "end_time": "2024-08-01T14:50:15.957197Z",
     "start_time": "2024-08-01T14:49:36.833410100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/001, Batch: 001, Batch Time: 192.04 ms\n",
      "Epoch: 001/001, Batch: 002, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 003, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 004, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 005, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 006, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 007, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 008, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 009, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 010, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 011, Batch Time: 72.01 ms\n",
      "Epoch: 001/001, Batch: 012, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 013, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 014, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 015, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 016, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 017, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 018, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 019, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 020, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 021, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 022, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 023, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 024, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 025, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 026, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 027, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 028, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 029, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 030, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 031, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 032, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 033, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 034, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 035, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 036, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 037, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 038, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 039, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 040, Batch Time: 85.02 ms\n",
      "Epoch: 001/001, Batch: 041, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 042, Batch Time: 73.01 ms\n",
      "Epoch: 001/001, Batch: 043, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 044, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 045, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 046, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 047, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 048, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 049, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 050, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 051, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 052, Batch Time: 77.02 ms\n",
      "Epoch: 001/001, Batch: 053, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 054, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 055, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 056, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 057, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 058, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 059, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 060, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 061, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 062, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 063, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 064, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 065, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 066, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 067, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 068, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 069, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 070, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 071, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 072, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 073, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 074, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 075, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 076, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 077, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 078, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 079, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 080, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 081, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 082, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 083, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 084, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 085, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 086, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 087, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 088, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 089, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 090, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 091, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 092, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 093, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 094, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 095, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 096, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 097, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 098, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 099, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 100, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 101, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 102, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 103, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 104, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 105, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 106, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 107, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 108, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 109, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 110, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 111, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 112, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 113, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 114, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 115, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 116, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 117, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 118, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 119, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 120, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 121, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 122, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 123, Batch Time: 71.01 ms\n",
      "Epoch: 001/001, Batch: 124, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 125, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 126, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 127, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 128, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 129, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 130, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 131, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 132, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 133, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 134, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 135, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 136, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 137, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 138, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 139, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 140, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 141, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 142, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 143, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 144, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 145, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 146, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 147, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 148, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 149, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 150, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 151, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 152, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 153, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 154, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 155, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 156, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 157, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 158, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 159, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 160, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 161, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 162, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 163, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 164, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 165, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 166, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 167, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 168, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 169, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 170, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 171, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 172, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 173, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 174, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 175, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 176, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 177, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 178, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 179, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 180, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 181, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 182, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 183, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 184, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 185, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 186, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 187, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 188, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 189, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 190, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 191, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 192, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 193, Batch Time: 77.02 ms\n",
      "Epoch: 001/001, Batch: 194, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 195, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 196, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 197, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 198, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 199, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 200, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 201, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 202, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 203, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 204, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 205, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 206, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 207, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 208, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 209, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 210, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 211, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 212, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 213, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 214, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 215, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 216, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 217, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 218, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 219, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 220, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 221, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 222, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 223, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 224, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 225, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 226, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 227, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 228, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 229, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 230, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 231, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 232, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 233, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 234, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 235, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 236, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 237, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 238, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 239, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 240, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 241, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 242, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 243, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 244, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 245, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 246, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 247, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 248, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 249, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 250, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 251, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 252, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 253, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 254, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 255, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 256, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 257, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 258, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 259, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 260, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 261, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 262, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 263, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 264, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 265, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 266, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 267, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 268, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 269, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 270, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 271, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 272, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 273, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 274, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 275, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 276, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 277, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 278, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 279, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 280, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 281, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 282, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 283, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 284, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 285, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 286, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 287, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 288, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 289, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 290, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 291, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 292, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 293, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 294, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 295, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 296, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 297, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 298, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 299, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 300, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 301, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 302, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 303, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 304, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 305, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 306, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 307, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 308, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 309, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 310, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 311, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 312, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 313, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 314, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 315, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 316, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 317, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 318, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 319, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 320, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 321, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 322, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 323, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 324, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 325, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 326, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 327, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 328, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 329, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 330, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 331, Batch Time: 76.02 ms\n",
      "Epoch: 001/001, Batch: 332, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 333, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 334, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 335, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 336, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 337, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 338, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 339, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 340, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 341, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 342, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 343, Batch Time: 85.02 ms\n",
      "Epoch: 001/001, Batch: 344, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 345, Batch Time: 71.02 ms\n",
      "Epoch: 001/001, Batch: 346, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 347, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 348, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 349, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 350, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 351, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 352, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 353, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 354, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 355, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 356, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 357, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 358, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 359, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 360, Batch Time: 83.02 ms\n",
      "Epoch: 001/001, Batch: 361, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 362, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 363, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 364, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 365, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 366, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 367, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 368, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 369, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 370, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 371, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 372, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 373, Batch Time: 74.02 ms\n",
      "Epoch: 001/001, Batch: 374, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 375, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 376, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 377, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 378, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 379, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 380, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 381, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 382, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 383, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 384, Batch Time: 73.02 ms\n",
      "Epoch: 001/001, Batch: 385, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 386, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 387, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 388, Batch Time: 75.02 ms\n",
      "Epoch: 001/001, Batch: 389, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 390, Batch Time: 72.02 ms\n",
      "Epoch: 001/001, Batch: 391, Batch Time: 82.02 ms\n",
      "Epoch: 001/001 completed.\n",
      "Total Training Time: 34.13 sec\n",
      "Total Accuracy: 65.71 %\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)\n",
    "simulate_key_press('p')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d}\\n')\n",
    "        \n",
    "    elapsed = (time.time() - start_time)\n",
    "    \n",
    "total_training_time = (time.time() - start_time)\n",
    "\n",
    "total_accuracy = compute_acc(model, valid_loader, device=DEVICE)\n",
    "print(f'Total Training Time: {total_training_time:.2f} sec')\n",
    "print(f'Total Training Time: {total_accuracy:.2f} %')\n",
    "time.sleep(1)\n",
    "simulate_key_press('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "paaeEQHQj5xC"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(77.1000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "#Zum loggen der Trainingslaufzeit und accuracy  des Models\n",
    "\n",
    "def add_row_to_csv(filename, datensatz_name, frequency, voltage, runtime, accuracy):\n",
    "    # Daten für die neue Zeile\n",
    "    new_row = {'Datensatz': datensatz_name, 'Frequenz': frequency, 'voltage': voltage, 'Laufzeit': runtime, 'Accuracy': accuracy}\n",
    "    \n",
    "    # Überprüfe, ob die Datei existiert\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    # Öffne die CSV-Datei im Modus 'a' (append), um Werte hinzuzufügen\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        # Erstelle einen CSV-Writer, wenn die Datei neu erstellt wird\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=['Datensatz', 'Frequenz', 'voltage', 'Laufzeit', 'Accuracy'])\n",
    "        \n",
    "        # Schreibe die Kopfzeile, falls die Datei neu erstellt wurde\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        # Schreibe die neue Zeile in die CSV-Datei\n",
    "        writer.writerow(new_row)\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "filename = 'densenet121_cifar_spannung.csv'\n",
    "datensatz_name = 'cifar'\n",
    "frequency = '3600'\n",
    "voltage = '1.1'\n",
    "runtime = round(total_training_time, 2)\n",
    "print(total_accuracy)\n",
    "accuracy = round(total_accuracy.item(), 2)\n",
    "\n",
    "add_row_to_csv(filename, datensatz_name, frequency, voltage, runtime, accuracy)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-06T12:43:27.120975400Z",
     "start_time": "2024-06-06T12:43:27.103971500Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
